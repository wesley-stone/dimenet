{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 18:44:03.659079: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.4.0 and strictly below 2.7.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import logging\n",
    "import string\n",
    "import random\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "\n",
    "from dimenet.model.dimenet import DimeNet\n",
    "from dimenet.model.dimenet_pp import DimeNetPP\n",
    "from dimenet.model.activations import swish\n",
    "from dimenet.training.trainer import Trainer\n",
    "from dimenet.training.metrics import Metrics\n",
    "from dimenet.training.data_container import DataContainer\n",
    "from dimenet.training.data_provider import DataProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "        fmt='%(asctime)s (%(levelname)s): %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S')\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel('INFO')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "tf.get_logger().setLevel('WARN')\n",
    "tf.autograph.set_verbosity(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.yaml for DimeNet, config_pp.yaml for DimeNet++\n",
    "with open('config_dye.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = config['model_name']\n",
    "\n",
    "if model_name == \"dimenet\":\n",
    "    num_bilinear = config['num_bilinear']\n",
    "elif model_name == \"dimenet++\":\n",
    "    out_emb_size = config['out_emb_size']\n",
    "    int_emb_size = config['int_emb_size']\n",
    "    basis_emb_size = config['basis_emb_size']\n",
    "    extensive = config['extensive']\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model name: '{model_name}'\")\n",
    "    \n",
    "emb_size = config['emb_size']\n",
    "num_blocks = config['num_blocks']\n",
    "\n",
    "num_spherical = config['num_spherical']\n",
    "num_radial = config['num_radial']\n",
    "output_init = config['output_init']\n",
    "\n",
    "cutoff = config['cutoff']\n",
    "envelope_exponent = config['envelope_exponent']\n",
    "\n",
    "num_before_skip = config['num_before_skip']\n",
    "num_after_skip = config['num_after_skip']\n",
    "num_dense_output = config['num_dense_output']\n",
    "\n",
    "num_train = config['num_train']\n",
    "num_valid = config['num_valid']\n",
    "data_seed = config['data_seed']\n",
    "dataset = config['dataset']\n",
    "logdir = config['logdir']\n",
    "\n",
    "num_steps = config['num_steps']\n",
    "ema_decay = config['ema_decay']\n",
    "\n",
    "learning_rate = config['learning_rate']\n",
    "warmup_steps = config['warmup_steps']\n",
    "decay_rate = config['decay_rate']\n",
    "decay_steps = config['decay_steps']\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "evaluation_interval = config['evaluation_interval']\n",
    "save_interval = config['save_interval']\n",
    "restart = config['restart']\n",
    "comment = config['comment']\n",
    "targets = config['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-30 18:58:28 (INFO): Directory: ./20211030_185828_BFExwfsF_dye_train.npz_homo-lumo_final\n"
     ]
    }
   ],
   "source": [
    "# Used for creating a random \"unique\" id for this run\n",
    "def id_generator(size=8, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits):\n",
    "    return ''.join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# Create directories\n",
    "# A unique directory name is created for this run based on the input\n",
    "if restart is None:\n",
    "    directory = (logdir + \"/\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_\" + id_generator()\n",
    "                 + \"_\" + os.path.basename(dataset)\n",
    "                 + \"_\" + '-'.join(targets)\n",
    "                 + \"_\" + comment)\n",
    "else:\n",
    "    directory = restart\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "best_dir = os.path.join(directory, 'best')\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, 'logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "best_loss_file = os.path.join(best_dir, 'best_loss.npz')\n",
    "best_ckpt_file = os.path.join(best_dir, 'ckpt')\n",
    "step_ckpt_folder = log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create summary writer and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "train = {}\n",
    "validation = {}\n",
    "\n",
    "train['metrics'] = Metrics('train', targets)\n",
    "validation['metrics'] = Metrics('val', targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['homo', 'lumo']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = np.load(dataset, allow_pickle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     1,     2, ..., 98523, 98524, 98525])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['homo', 'lumo']\n"
     ]
    }
   ],
   "source": [
    "data_container = DataContainer(dataset, cutoff=cutoff, target_keys=targets)\n",
    "\n",
    "# Initialize DataProvider (splits dataset into 3 sets based on data_seed and provides tf.datasets)\n",
    "data_provider = DataProvider(data_container, num_train, num_valid, batch_size,\n",
    "                             seed=data_seed, randomized=True)\n",
    "\n",
    "# Initialize datasets\n",
    "train['dataset'] = data_provider.get_dataset('train').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "train['dataset_iter'] = iter(train['dataset'])\n",
    "validation['dataset'] = data_provider.get_dataset('val').prefetch(tf.data.experimental.AUTOTUNE)\n",
    "validation['dataset_iter'] = iter(validation['dataset'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"dimenet\":\n",
    "    model = DimeNet(\n",
    "            emb_size=emb_size, num_blocks=num_blocks, num_bilinear=num_bilinear,\n",
    "            num_spherical=num_spherical, num_radial=num_radial,\n",
    "            cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "            num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "            num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "            activation=swish, output_init=output_init)\n",
    "elif model_name == \"dimenet++\":\n",
    "    model = DimeNetPP(\n",
    "            emb_size=emb_size, out_emb_size=out_emb_size,\n",
    "            int_emb_size=int_emb_size, basis_emb_size=basis_emb_size,\n",
    "            num_blocks=num_blocks, num_spherical=num_spherical, num_radial=num_radial,\n",
    "            cutoff=cutoff, envelope_exponent=envelope_exponent,\n",
    "            num_before_skip=num_before_skip, num_after_skip=num_after_skip,\n",
    "            num_dense_output=num_dense_output, num_targets=len(targets),\n",
    "            activation=swish, extensive=extensive, output_init=output_init)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown model name: '{model_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/load best recorded loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenshi/dimenet/dimenet/training/metrics.py:68: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.mean(np.log(self.maes)).item()\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(best_loss_file):\n",
    "    loss_file = np.load(best_loss_file)\n",
    "    metrics_best = {k: v.item() for k, v in loss_file.items()}\n",
    "else:\n",
    "    metrics_best = validation['metrics'].result()\n",
    "    for key in metrics_best.keys():\n",
    "        metrics_best[key] = np.inf\n",
    "    metrics_best['step'] = 0\n",
    "    np.savez(best_loss_file, **metrics_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, learning_rate, warmup_steps,\n",
    "                  decay_steps, decay_rate,\n",
    "                  ema_decay=ema_decay, max_grad_norm=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up checkpointing and load latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up checkpointing\n",
    "ckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=trainer.optimizer, model=model)\n",
    "manager = tf.train.CheckpointManager(ckpt, step_ckpt_folder, max_to_keep=3)\n",
    "\n",
    "# Restore latest checkpoint\n",
    "ckpt_restored = tf.train.latest_checkpoint(log_dir)\n",
    "if ckpt_restored is not None:\n",
    "    ckpt.restore(ckpt_restored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "Note that the warning `UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.` is expected. It is due to the backward pass of `tf.gather` ([this one](https://github.com/klicperajo/dimenet/blob/master/dimenet/model/layers/interaction_block.py#L62), to be exact) producing sparse gradients, which the previous layer has to convert to a dense tensor (see [this Stack Overflow question](https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:431: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "2021-10-30 19:07:17.636789: W tensorflow/core/framework/op_kernel.cc:1755] Invalid argument: ValueError: need at least one array to concatenate\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 827, in generator_py_func\n",
      "    values = next(generator_state.get_iterator(iterator_id))\n",
      "\n",
      "  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 94, in generator\n",
      "    yield self.idx_to_data(idx)\n",
      "\n",
      "  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 74, in idx_to_data\n",
      "    batch = self.data_container[idx]\n",
      "\n",
      "  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 75, in __getitem__\n",
      "    adj_matrix = self._bmat_fast(adj_matrices)\n",
      "\n",
      "  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 28, in _bmat_fast\n",
      "    new_data = np.concatenate([mat.data for mat in mats])\n",
      "\n",
      "  File \"<__array_function__ internals>\", line 5, in concatenate\n",
      "\n",
      "ValueError: need at least one array to concatenate\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "2 root error(s) found.\n  (0) Invalid argument:  ValueError: need at least one array to concatenate\nTraceback (most recent call last):\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 827, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 94, in generator\n    yield self.idx_to_data(idx)\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 74, in idx_to_data\n    batch = self.data_container[idx]\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 75, in __getitem__\n    adj_matrix = self._bmat_fast(adj_matrices)\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 28, in _bmat_fast\n    new_data = np.concatenate([mat.data for mat in mats])\n\n  File \"<__array_function__ internals>\", line 5, in concatenate\n\nValueError: need at least one array to concatenate\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Invalid argument:  ValueError: need at least one array to concatenate\nTraceback (most recent call last):\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 827, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 94, in generator\n    yield self.idx_to_data(idx)\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 74, in idx_to_data\n    batch = self.data_container[idx]\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 75, in __getitem__\n    adj_matrix = self._bmat_fast(adj_matrices)\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 28, in _bmat_fast\n    new_data = np.concatenate([mat.data for mat in mats])\n\n  File \"<__array_function__ internals>\", line 5, in concatenate\n\nValueError: need at least one array to concatenate\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[gradient_tape/dimenet/interaction/Size_3/_24]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_on_batch_57183]\n\nFunction call stack:\ntrain_on_batch -> train_on_batch\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19677/985606364.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Perform training step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset_iter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Save progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument:  ValueError: need at least one array to concatenate\nTraceback (most recent call last):\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 827, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 94, in generator\n    yield self.idx_to_data(idx)\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 74, in idx_to_data\n    batch = self.data_container[idx]\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 75, in __getitem__\n    adj_matrix = self._bmat_fast(adj_matrices)\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 28, in _bmat_fast\n    new_data = np.concatenate([mat.data for mat in mats])\n\n  File \"<__array_function__ internals>\", line 5, in concatenate\n\nValueError: need at least one array to concatenate\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n  (1) Invalid argument:  ValueError: need at least one array to concatenate\nTraceback (most recent call last):\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/ops/script_ops.py\", line 244, in __call__\n    ret = func(*args)\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\", line 302, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/home/wenshi/anaconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 827, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 94, in generator\n    yield self.idx_to_data(idx)\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_provider.py\", line 74, in idx_to_data\n    batch = self.data_container[idx]\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 75, in __getitem__\n    adj_matrix = self._bmat_fast(adj_matrices)\n\n  File \"/home/wenshi/dimenet/dimenet/training/data_container.py\", line 28, in _bmat_fast\n    new_data = np.concatenate([mat.data for mat in mats])\n\n  File \"<__array_function__ internals>\", line 5, in concatenate\n\nValueError: need at least one array to concatenate\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[gradient_tape/dimenet/interaction/Size_3/_24]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_on_batch_57183]\n\nFunction call stack:\ntrain_on_batch -> train_on_batch\n"
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "    if ckpt_restored is not None:\n",
    "        step_init = ckpt.step.numpy()\n",
    "    else:\n",
    "        step_init = 1\n",
    "    for step in range(step_init, num_steps + 1):\n",
    "        # Update step number\n",
    "        ckpt.step.assign(step)\n",
    "        tf.summary.experimental.set_step(step)\n",
    "\n",
    "        # Perform training step\n",
    "        trainer.train_on_batch(train['dataset_iter'], train['metrics'])\n",
    "\n",
    "        # Save progress\n",
    "        if (step % save_interval == 0):\n",
    "            manager.save()\n",
    "\n",
    "        # Evaluate model and log results\n",
    "        if (step % evaluation_interval == 0):\n",
    "\n",
    "            # Save backup variables and load averaged variables\n",
    "            trainer.save_variable_backups()\n",
    "            trainer.load_averaged_variables()\n",
    "\n",
    "            # Compute results on the validation set\n",
    "            for i in range(int(np.ceil(num_valid / batch_size))):\n",
    "                trainer.test_on_batch(validation['dataset_iter'], validation['metrics'])\n",
    "\n",
    "            # Update and save best result\n",
    "            if validation['metrics'].mean_mae < metrics_best['mean_mae_val']:\n",
    "                metrics_best['step'] = step\n",
    "                metrics_best.update(validation['metrics'].result())\n",
    "\n",
    "                np.savez(best_loss_file, **metrics_best)\n",
    "                model.save_weights(best_ckpt_file)\n",
    "\n",
    "            for key, val in metrics_best.items():\n",
    "                if key != 'step':\n",
    "                    tf.summary.scalar(key + '_best', val)\n",
    "\n",
    "            epoch = step // steps_per_epoch\n",
    "            logging.info(\n",
    "                f\"{step}/{num_steps} (epoch {epoch+1}): \"\n",
    "                f\"Loss: train={train['metrics'].loss:.6f}, val={validation['metrics'].loss:.6f}; \"\n",
    "                f\"logMAE: train={train['metrics'].mean_log_mae:.6f}, \"\n",
    "                f\"val={validation['metrics'].mean_log_mae:.6f}\")\n",
    "\n",
    "            train['metrics'].write()\n",
    "            validation['metrics'].write()\n",
    "\n",
    "            train['metrics'].reset_states()\n",
    "            validation['metrics'].reset_states()\n",
    "\n",
    "            # Restore backup variables\n",
    "            trainer.restore_variable_backups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
